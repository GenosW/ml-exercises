{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moneyball - Baseball Dataset\n",
    "\n",
    "URL: https://www.openml.org/d/41021\n",
    "\n",
    "## Content\n",
    "\n",
    "1) [Data preprocessing](#dataproc)\n",
    "\n",
    "2) [Model training](#train)\n",
    "    \n",
    "2.a) [Linear regression](#linear)\n",
    "\n",
    "2.b) [Lasso Regression](#lasso)\n",
    "\n",
    "2.c) [Random Forest](#rf)\n",
    "\n",
    "2.d) [kNN](#knn)\n",
    "\n",
    "3) Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# models for linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "\n",
    "# models for Lasso regression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# statistic tools\n",
    "from sklearn import metrics\n",
    "from statistics import stdev\n",
    "\n",
    "# preprocessing\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataproc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'baseball.csv'\n",
    "df_raw = pd.read_csv(input_file,  sep = ',', header = 0)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of data columns\n",
    "\n",
    "RS ... Runs Scored, \n",
    "\n",
    "RA ... Runs Allowed\n",
    "\n",
    "***RD ... Run differential (actually difference)***\n",
    "\n",
    "W ... Wins\n",
    "\n",
    "OBP ... On-Base Percentage\n",
    "\n",
    "SLG ... Slugging Percentage\n",
    "\n",
    "BA ... Batting Average\n",
    "\n",
    "Playoffs (binary)\n",
    "\n",
    "RankSeason\n",
    "\n",
    "RankPlayoffs\n",
    "\n",
    "G ... Games Played\n",
    "\n",
    "OOBP ... Opponent On-Base Percentage\n",
    "\n",
    "OSLG ... Opponent Slugging Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dict = {'RS':  'Runs Scored', \n",
    "            'RA':  'Runs Allowed',\n",
    "            'RD':  'Run differential (actually difference)',\n",
    "            'W':  'Wins',\n",
    "            'OBP':  'On-Base Percentage',\n",
    "            'SLG':  'Slugging Percentage',\n",
    "            'BA':  'Batting Average',\n",
    "            'Playoffs': 'playoffs reached (binary)',\n",
    "            'RankSeason': 'season rank',\n",
    "            'RankPlayoffs': 'playoff rank',\n",
    "            'G':  'Games Played',\n",
    "            'OOBP':  'Opponent On-Base Percentage',\n",
    "            'OSLG':  'Opponent Slugging Percentage'\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000): \n",
    "        with pd.option_context(\"display.max_columns\", 1000): \n",
    "            display(df)\n",
    "            \n",
    "def add_RD(df):\n",
    "    df['RD'] = df.apply(lambda row: row.RS - row.RA, axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First look on DATA and information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw.tail().transpose())\n",
    "print('#'*40)\n",
    "display('Some more info')\n",
    "print('#'*40)\n",
    "display(df_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "def split_simple(df, n): \n",
    "    '''n... number to split at'''\n",
    "    return df[:n].copy(), df[n:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = df_raw\n",
    "add_RD(df_prep) # add Round Difference\n",
    "display_all(df_prep.tail().transpose())\n",
    "display(df_prep.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Team', 'League', 'Year', 'RankSeason', 'RankPlayoffs', 'Playoffs']\n",
    "df_prep = df_prep.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Fix missing values and type\n",
    "df_prep.replace(\"?\",0, inplace=True)\n",
    "#df_prep = df_prep[df_prep.OOBP != 0]\n",
    "df_prep[['OOBP','OSLG']] = df_prep[['OOBP','OSLG']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_prep.columns.values)\n",
    "display(df_prep.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf = df_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping:\n",
    "\n",
    "Bootstrapping: Selecting data from a data to generate a new dataset of the same size by picking WITH replacement.\n",
    "\n",
    "Example:\n",
    "\n",
    "    > DS = [1,2,3,4]\n",
    "    > could turn into \n",
    "    > DS_bootstrapped = [3,2,4,4]\n",
    "    \n",
    "Consequences:\n",
    "\n",
    "- Instances (rows) of the original set can end up duplicated (multiple times) in the resulting dataset.\n",
    "- Some instances are left out entirely (up to 1/3) --> \"Out-Of-Bag Dataset\" (=OOB Dataset)\n",
    "\n",
    "## Using the OOB Dataset\n",
    "\n",
    "The OOB dataset was not used to construct the tree, so we can actually use it to test our tree and gain some insight into the error measure of the tree.\n",
    "This error is called the \"Out-Of-Bag Error\" (OOB error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing LinReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.lmplot(\"RS\",\"W\",df_prep)\n",
    "\n",
    "sns.lmplot(\"RA\",\"W\",df_prep)\n",
    "\n",
    "sns.lmplot(\"OBP\",\"W\",df_prep)\n",
    "\n",
    "sns.lmplot(\"SLG\",\"W\",df_prep)\n",
    "\n",
    "sns.lmplot(\"BA\",\"W\",df_prep)\n",
    "df_lin = df_prep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing LassoReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Model training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linear'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_lin[['W']]\n",
    "X = df_lin[['RS','RA','OBP','SLG','BA']]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression(normalize = True)\n",
    "linreg.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.coef_\n",
    "score = linreg.score(X_test,Y_test)\n",
    "print(\"score: \",score)\n",
    "Y_lin_pred = linreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lasso'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(normalize = True)\n",
    "parameters = {'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,2,5,10,20,30,35,40,45,50,55,100]}\n",
    "lasso_regressor = GridSearchCV(lasso,parameters,scoring = 'neg_mean_squared_error',cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_regressor.fit(X_train,Y_train)\n",
    "print(lasso_regressor.best_params_)\n",
    "print(lasso_regressor.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_lasso_pred = lasso_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rf'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def rmse(x,y): \n",
    "    return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m, X_train, X_valid, y_train, y_valid, score='neg_mean_squared_error'):\n",
    "    res = {\n",
    "        'RMS(train)': rmse(m.predict(X_train), y_train),\n",
    "        'RMS(valid)': rmse(m.predict(X_valid), y_valid)}\n",
    "    if score=='neg_mean_squared_error':\n",
    "        res['Model_Score=r²'] = [np.sqrt(-m.score(X_train, y_train)), np.sqrt(-m.score(X_valid, y_valid))]\n",
    "    elif score=='pos_mean_squared_error':\n",
    "        res['Model_Score=r²'] = [np.sqrt(m.score(X_train, y_train)), np.sqrt(m.score(X_valid, y_valid))]\n",
    "    else:\n",
    "        res['Model_Score=r²'] = [m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res['oob_score_'] = m.oob_score_\n",
    "    display(res)\n",
    "    return res\n",
    "\n",
    "# Feature importance\n",
    "from prettytable import PrettyTable as PT # pip install PTable\n",
    "def print_RF_featureImportance(rf, X):\n",
    "    table = PT()\n",
    "    table.field_names = ['Feature', 'Score', 'Comment']\n",
    "    for name, score in zip(X.columns.values, rf.feature_importances_):\n",
    "        print(f\"{name}: {score:.5f}\\t\\t... {col_dict[name]}\")\n",
    "        table.add_row([name, round(score, ndigits=4), col_dict[name]])\n",
    "    print(table)\n",
    "\n",
    "def print_GridSearchResult(grid):\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split for random forest\n",
    "df = df_rf\n",
    "rnd_state = 42\n",
    "ratio = 0.2 # test/num_samples\n",
    "#####\n",
    "num_instances, _ = df.shape\n",
    "print(f\"From {num_instances} using {num_instances*ratio:.0f} for testing and {num_instances*(1-ratio):.0f} for training. Ratio = {ratio*100:.2f}%\")\n",
    "X, y = (d.drop(['W', 'RD'], axis=1), df.W)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = ratio, random_state = rnd_state)\n",
    "display(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training of RFRegressor\n",
    "n_cores = 4\n",
    "rf_W = RandomForestRegressor(n_jobs=n_cores)\n",
    "# The following code is supposed to fail due to string values in the input data\n",
    "rf_W.fit(X_train, y_train)\n",
    "print(\"Before:\")\n",
    "display(before)#\n",
    "print(\"Now:\")\n",
    "before = print_score(rf_W, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_RF_featureImportance(rf_W, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_W_prediction = rf_W.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test-rf_W_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 4\n",
    "number_of_trees = 1000 # default = 100\n",
    "rf = RandomForestRegressor(n_jobs=n_cores, n_estimators=number_of_trees, bootstrap=True) #, verbose=1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Before:\")\n",
    "display(before)#\n",
    "print(\"Now:\")\n",
    "before = print_score(rf, X_train, X_test, y_train, y_test)\n",
    "print()\n",
    "print(\"Feature importance\")\n",
    "print_RF_featureImportance(rf, X_train)\n",
    "rf_RD = rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfRD_prediction = rf_RD.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test-rfRD_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Hyperparameters via GridSearch\n",
    "\n",
    "because we lazy bois\n",
    "\n",
    "## Notes on the RandomForestRegressor from scikit-learn\n",
    "-----\n",
    "The default values for the parameters controlling the size of the trees\n",
    "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "unpruned trees which can potentially be very large on some data sets. To\n",
    "reduce memory consumption, the complexity and size of the trees should be\n",
    "controlled by setting those parameter values.\n",
    "\n",
    "## Number of variables/features per tree --> 'max_features'\n",
    "\n",
    "A good starting point is/might be: *the square root of the number of features presented to the tree*. Then, test some values below and above that starting point.\n",
    "\n",
    "## Number of trees in the forest --> 'n_estimators'\n",
    "\n",
    "The more the merrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt\n",
    "num_features = X.shape[1]\n",
    "print(num_features)\n",
    "sqrt_num_features = round(sqrt(num_features), 0)\n",
    "sqrt_num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "n_cores = 4\n",
    "# but since we dont have that many features...we are just gonna brute force it :D\n",
    "param_grid = [\n",
    "    {\n",
    "        'n_estimators': [3, 10, 30, 100, 1000], 'max_features': [i for i in range(1,num_features+1)]\n",
    "    }\n",
    "#,{'bootstrap': [False], 'n_estimators': [3, 30, 100, 1000], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "k = 10\n",
    "forest_reg = RandomForestRegressor(n_jobs=n_cores)\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, n_jobs=n_cores , cv=k, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_GridSearchResult(grid_search)\n",
    "grid_search.scorer_()\n",
    "scores = grid_search.score(X_test, y_test)\n",
    "print_score(grid_search, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Run', 'Score']\n",
    "    for i, score in enumerate(scores):\n",
    "        table.add_row([i, round(score, 3)])\n",
    "    print(table)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "model = rf_RD\n",
    "scores = cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(rf_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(Y_train, Y_lin_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(Y_train, Y_lin_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_train, Y_lin_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Y_train)\n",
    "sns.distplot(Y_lin_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Y_train)\n",
    "sns.distplot(Y_lasso_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geht leider noch nicht die dimesionen passen nicht zusammen\n",
    "#print('Mean Absolute Error:', metrics.mean_absolute_error(Y_train, Y_lasso_pred))  \n",
    "#print('Mean Squared Error:', metrics.mean_squared_error(Y_train, Y_lasso_pred))  \n",
    "#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_train, Y_lasso_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump model\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "joblib.dump(rf_RD, \"tmp/rf_RD.pkl\")\n",
    "# To load the model\n",
    "# my_model_loaded = joblib.load(\"my_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "df_raw.to_feather('tmp/bulldozers-raw')\n",
    "df_raw = pd.read_feather('tmp/raw')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bitb5f188b5fd4549769c9ea1e1a366fa0d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
