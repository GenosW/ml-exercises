{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moneyball - Baseball Dataset\n",
    "\n",
    "URL: https://www.openml.org/d/41021\n",
    "\n",
    "## Content\n",
    "\n",
    "1) [Data preprocessing](#dataproc)\n",
    "\n",
    "2) [Model training and evaluation](#train) \n",
    "    \n",
    "2.a) [Linear regression](#linear)\n",
    "\n",
    "2.b) [Lasso Regression](#lasso)\n",
    "\n",
    "2.c) [Random Forest](#rf)\n",
    "\n",
    "2.d) [kNN](#knn)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# models for linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "\n",
    "# models for Lasso regression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# statistic tools\n",
    "from sklearn import metrics\n",
    "from statistics import stdev\n",
    "\n",
    "# preprocessing\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'baseball.csv'\n",
    "df_raw = pd.read_csv(input_file,  sep = ',', header = 0)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of data columns\n",
    "\n",
    "RS ... Runs Scored, \n",
    "\n",
    "RA ... Runs Allowed\n",
    "\n",
    "***RD ... Run differential (actually difference)***\n",
    "\n",
    "W ... Wins\n",
    "\n",
    "OBP ... On-Base Percentage\n",
    "\n",
    "SLG ... Slugging Percentage\n",
    "\n",
    "BA ... Batting Average\n",
    "\n",
    "Playoffs (binary)\n",
    "\n",
    "RankSeason\n",
    "\n",
    "RankPlayoffs\n",
    "\n",
    "G ... Games Played\n",
    "\n",
    "OOBP ... Opponent On-Base Percentage\n",
    "\n",
    "OSLG ... Opponent Slugging Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dict = {'RS':  'Runs Scored', \n",
    "            'RA':  'Runs Allowed',\n",
    "            'RD':  'Run differential (actually difference)',\n",
    "            'W':  'Wins',\n",
    "            'OBP':  'On-Base Percentage',\n",
    "            'SLG':  'Slugging Percentage',\n",
    "            'BA':  'Batting Average',\n",
    "            'Playoffs': 'playoffs reached (binary)',\n",
    "            'RankSeason': 'season rank',\n",
    "            'RankPlayoffs': 'playoff rank',\n",
    "            'G':  'Games Played',\n",
    "            'OOBP':  'Opponent On-Base Percentage',\n",
    "            'OSLG':  'Opponent Slugging Percentage'\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000): \n",
    "        with pd.option_context(\"display.max_columns\", 1000): \n",
    "            display(df)\n",
    "            \n",
    "def add_RD(df):\n",
    "    df['RD'] = df.apply(lambda row: row.RS - row.RA, axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First look on DATA and information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw.tail().transpose())\n",
    "print('#'*40)\n",
    "display('Some more info')\n",
    "print('#'*40)\n",
    "display(df_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "def split_simple(df, n): \n",
    "    '''n... number to split at'''\n",
    "    return df[:n].copy(), df[n:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = df_raw\n",
    "add_RD(df_prep) # add Round Difference\n",
    "display_all(df_prep.tail().transpose())\n",
    "display(df_prep.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Team', 'League', 'Year', 'RankSeason', 'RankPlayoffs', 'Playoffs']\n",
    "df_prep = df_prep.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Fix missing values and type\n",
    "df_prep.replace(\"?\",0, inplace=True)\n",
    "#df_prep = df_prep[df_prep.OOBP != 0]\n",
    "df_prep[['OOBP','OSLG']] = df_prep[['OOBP','OSLG']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_prep.columns.values)\n",
    "display(df_prep.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf = df_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping:\n",
    "\n",
    "Bootstrapping: Selecting data from a data to generate a new dataset of the same size by picking WITH replacement.\n",
    "\n",
    "Example:\n",
    "\n",
    "    > DS = [1,2,3,4]\n",
    "    > could turn into \n",
    "    > DS_bootstrapped = [3,2,4,4]\n",
    "    \n",
    "Consequences:\n",
    "\n",
    "- Instances (rows) of the original set can end up duplicated (multiple times) in the resulting dataset.\n",
    "- Some instances are left out entirely (up to 1/3) --> \"Out-Of-Bag Dataset\" (=OOB Dataset)\n",
    "\n",
    "## Using the OOB Dataset\n",
    "\n",
    "The OOB dataset was not used to construct the tree, so we can actually use it to test our tree and gain some insight into the error measure of the tree.\n",
    "This error is called the \"Out-Of-Bag Error\" (OOB error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing LinReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.lmplot(\"RS\",\"W\",df_prep)\n",
    "\n",
    "sns.lmplot(\"RA\",\"W\",df_prep)\n",
    "\n",
    "sns.lmplot(\"OBP\",\"W\",df_prep)\n",
    "\n",
    "sns.lmplot(\"SLG\",\"W\",df_prep)\n",
    "\n",
    "sns.lmplot(\"BA\",\"W\",df_prep)\n",
    "df_lin = df_prep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing LassoReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No special preprocessing for LassoReg needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df_knn = df_raw\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute the missing values within the OOBP and OSLG columns - with 0.3 being the relative mean value\n",
    "imputerQ1 = SimpleImputer(missing_values='?', strategy='constant', fill_value=0.3)\n",
    "imputerQ1 = imputerQ1.fit(df_knn[['OOBP', 'OSLG']])\n",
    "df_knn[['OOBP', 'OSLG']] = imputerQ1.transform(df_knn[['OOBP', 'OSLG']])\n",
    "\n",
    "# Impute the missing values within the RankSeason column - the idea behind 6 is that every team which isn't in the playoffs ranked worse than Rank 5\n",
    "imputerQ2 = SimpleImputer(missing_values='?', strategy='constant', fill_value=6)\n",
    "imputerQ2 = imputerQ2.fit(df_knn[['RankSeason']])\n",
    "df_knn[['RankSeason']] = imputerQ2.transform(df_knn[['RankSeason']])\n",
    "df_knn.League.replace(['NL', 'AL'], [1, 0], inplace=True)\n",
    "\n",
    "#Drop useless columns - of no interest\n",
    "df_knn.drop(['RankPlayoffs', 'Team', 'Year'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(df_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2) Model training and evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_lin[['W']]\n",
    "X = df_lin[['RS','RA','OBP','SLG','BA']]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression(normalize = True)\n",
    "linreg.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.coef_\n",
    "score = linreg.score(X_test,Y_test)\n",
    "print(\"Linear regression model score: \",score)\n",
    "Y_lin_pred = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_lin_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_lin_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_lin_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Y_test)\n",
    "sns.distplot(Y_lin_pred, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Y_test-Y_lin_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(normalize = True)\n",
    "parameters = {'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,2,5,10,20,30,35,40,45,50,55,100]}\n",
    "lasso_regressor = GridSearchCV(lasso,parameters,scoring = 'neg_mean_squared_error',cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_regressor.fit(X_train,Y_train)\n",
    "print(\"Lasso regression model score: \", lasso_regressor.score(X_test, Y_test))\n",
    "print(lasso_regressor.best_params_)\n",
    "print(lasso_regressor.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_lasso_pred = lasso_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Y_test)\n",
    "sns.distplot(Y_lasso_pred, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geht leider noch nicht die dimesionen passen nicht zusammen\n",
    "#print('Mean Absolute Error:', metrics.mean_absolute_error(Y_train, Y_lasso_pred))  \n",
    "#print('Mean Squared Error:', metrics.mean_squared_error(Y_train, Y_lasso_pred))  \n",
    "#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_train, Y_lasso_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_knn, test_knn = train_test_split(df_knn, test_size=0.3)\n",
    "\n",
    "x_train_knn = train_knn.drop('W', axis=1)\n",
    "y_train_knn = train_knn['W']\n",
    "\n",
    "x_test_knn = test_knn.drop('W', axis=1)\n",
    "y_test_knn = test_knn['W']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "x_train_knn_scaled = scaler.fit_transform(x_train_knn)\n",
    "x_train_knn = pd.DataFrame(x_train_knn_scaled)\n",
    "\n",
    "x_test_knn_scaled = scaler.fit_transform(x_test_knn)\n",
    "x_test_knn = pd.DataFrame(x_test_knn_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rf'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_val_knn = [] # to store rmse values for different k\n",
    "for k in range(25):\n",
    "    k = k + 1\n",
    "    model = neighbors.KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(x_train_knn, y_train_knn)\n",
    "    pred = model.predict(x_test_knn)\n",
    "    error = sqrt(mean_squared_error(y_test_knn, pred))\n",
    "    rmse_val_knn.append(error)\n",
    "    print(\"RMSE for k={}: {}\".format(k, error))\n",
    "    print(\"R^2 for k={}: {}\\n\".format(k, model.score(x_test_knn, y_test_knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(range(1,26), rmse_val_knn, color='blue', linestyle='dashed', marker='o',\n",
    "        markerfacecolor='red', markersize=5)\n",
    "plt.title('RMSE vs. k-Value')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing kNN-search for optimal k-Value via Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'n_neighbors': range(1, 25)}\n",
    "\n",
    "knn = neighbors.KNeighborsRegressor()\n",
    "\n",
    "model = GridSearchCV(knn, params, cv=100)\n",
    "model.fit(x_train_knn, y_train_knn)\n",
    "print(\"Best k-Value is: \", model.best_params_['n_neighbors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv = neighbors.KNeighborsRegressor(n_neighbors=model.best_params_['n_neighbors'])\n",
    "model_cv.fit(x_train_knn, y_train_knn)\n",
    "pred_cv = model.predict(x_test_knn)\n",
    "sns.distplot(y_test_knn)\n",
    "sns.distplot(pred_cv, color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test_knn-pred_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def rmse(x,y): \n",
    "    return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m, X_train, X_valid, y_train, y_valid, score='neg_mean_squared_error'):\n",
    "    res = {\n",
    "        'RMS(train)': rmse(m.predict(X_train), y_train),\n",
    "        'RMS(valid)': rmse(m.predict(X_valid), y_valid)}\n",
    "    if score=='neg_mean_squared_error':\n",
    "        res['Model_Score=r²'] = [np.sqrt(-m.score(X_train, y_train)), np.sqrt(-m.score(X_valid, y_valid))]\n",
    "    elif score=='pos_mean_squared_error':\n",
    "        res['Model_Score=r²'] = [np.sqrt(m.score(X_train, y_train)), np.sqrt(m.score(X_valid, y_valid))]\n",
    "    else:\n",
    "        res['Model_Score=r²'] = [m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res['oob_score_'] = m.oob_score_\n",
    "    display(res)\n",
    "    return res\n",
    "\n",
    "# Feature importance\n",
    "from prettytable import PrettyTable as PT # pip install PTable\n",
    "def print_RF_featureImportance(rf, X):\n",
    "    table = PT()\n",
    "    table.field_names = ['Feature', 'Score', 'Comment']\n",
    "    for name, score in zip(X.columns.values, rf.feature_importances_):\n",
    "        print(f\"{name}: {score:.5f}\\t\\t... {col_dict[name]}\")\n",
    "        table.add_row([name, round(score, ndigits=4), col_dict[name]])\n",
    "    print(table)\n",
    "\n",
    "def print_GridSearchResult(grid):\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split for random forest\n",
    "rnd_state = 42\n",
    "ratio = 0.2 # test/num_samples\n",
    "#####\n",
    "num_instances, _ = df_rf.shape\n",
    "print(f\"From {num_instances} using {num_instances*ratio:.0f} for testing and {num_instances*(1-ratio):.0f} for training. Ratio = {ratio*100:.2f}%\")\n",
    "X, y = (df_rf.drop(['W', 'RD'], axis=1), df_rf.W)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = ratio, random_state = rnd_state)\n",
    "display(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training of RFRegressor\n",
    "n_cores = 4\n",
    "rf_W = RandomForestRegressor(n_jobs=n_cores)\n",
    "# The following code is supposed to fail due to string values in the input data\n",
    "rf_W.fit(X_train, y_train)\n",
    "print(\"Before:\")\n",
    "display(before)#\n",
    "print(\"Now:\")\n",
    "before = print_score(rf_W, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_RF_featureImportance(rf_W, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_W_prediction = rf_W.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test)\n",
    "sns.distplot(rf_W_prediction, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test-rf_W_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 4\n",
    "number_of_trees = 1000 # default = 100\n",
    "rf = RandomForestRegressor(n_jobs=n_cores, n_estimators=number_of_trees, bootstrap=True) #, verbose=1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Before:\")\n",
    "display(before)#\n",
    "print(\"Now:\")\n",
    "before = print_score(rf, X_train, X_test, y_train, y_test)\n",
    "print()\n",
    "print(\"Feature importance\")\n",
    "print_RF_featureImportance(rf, X_train)\n",
    "rf_RD = rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfRD_prediction = rf_RD.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test)\n",
    "sns.distplot(rfRD_prediction, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test-rfRD_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Hyperparameters via GridSearch\n",
    "\n",
    "because we lazy bois\n",
    "\n",
    "## Notes on the RandomForestRegressor from scikit-learn\n",
    "-----\n",
    "The default values for the parameters controlling the size of the trees\n",
    "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "unpruned trees which can potentially be very large on some data sets. To\n",
    "reduce memory consumption, the complexity and size of the trees should be\n",
    "controlled by setting those parameter values.\n",
    "\n",
    "## Number of variables/features per tree --> 'max_features'\n",
    "\n",
    "A good starting point is/might be: *the square root of the number of features presented to the tree*. Then, test some values below and above that starting point.\n",
    "\n",
    "## Number of trees in the forest --> 'n_estimators'\n",
    "\n",
    "The more the merrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt\n",
    "num_features = X.shape[1]\n",
    "print(num_features)\n",
    "sqrt_num_features = round(sqrt(num_features), 0)\n",
    "sqrt_num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "n_cores = 4\n",
    "# but since we dont have that many features...we are just gonna brute force it :D\n",
    "param_grid = [\n",
    "    {\n",
    "        'n_estimators': [3, 10, 30, 100, 1000], 'max_features': [i for i in range(1,num_features+1)]\n",
    "    }\n",
    "#,{'bootstrap': [False], 'n_estimators': [3, 30, 100, 1000], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "k = 10\n",
    "forest_reg = RandomForestRegressor(n_jobs=n_cores)\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, n_jobs=n_cores , cv=k, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_GridSearchResult(grid_search)\n",
    "grid_search.scorer_()\n",
    "scores = grid_search.score(X_test, y_test)\n",
    "print_score(grid_search, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Save model and DF\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump model\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "joblib.dump(rf_RD, \"tmp/rf_RD.pkl\")\n",
    "# To load the model\n",
    "# my_model_loaded = joblib.load(\"my_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "df_raw.to_feather('tmp/bulldozers-raw')\n",
    "df_raw = pd.read_feather('tmp/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit24de9dcde5da4d9f837ea055140cd15f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}