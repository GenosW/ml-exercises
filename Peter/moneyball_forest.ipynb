{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Moneyball dataset\n",
    "\n",
    "## a) [Linear regression](#linear)\n",
    "\n",
    "## b) [Lasso Regression](#lasso)\n",
    "\n",
    "## c) [Random Forest](#rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the first dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'baseball.csv'\n",
    "Mb_data = pd.read_csv(input_file,  sep = ',', header = 0)\n",
    "Mb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns\n",
    "\n",
    "RS ... Runs Scored, \n",
    "\n",
    "RA ... Runs Allowed\n",
    "\n",
    "***RD ... Run differential (actually difference)***\n",
    "\n",
    "W ... Wins\n",
    "\n",
    "OBP ... On-Base Percentage\n",
    "\n",
    "SLG ... Slugging Percentage\n",
    "\n",
    "BA ... Batting Average\n",
    "\n",
    "Playoffs (binary)\n",
    "\n",
    "RankSeason\n",
    "\n",
    "RankPlayoffs\n",
    "\n",
    "G ... Games Played\n",
    "\n",
    "OOBP ... Opponent On-Base Percentage\n",
    "\n",
    "OSLG ... Opponent Slugging Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dict = {'RS':  'Runs Scored', \n",
    "            'RA':  'Runs Allowed',\n",
    "            'RD':  'Run differential (actually difference)',\n",
    "            'W':  'Wins',\n",
    "            'OBP':  'On-Base Percentage',\n",
    "            'SLG':  'Slugging Percentage',\n",
    "            'BA':  'Batting Average',\n",
    "            'Playoffs': 'playoffs reached (binary)',\n",
    "            'RankSeason': 'season rank',\n",
    "            'RankPlayoffs': 'playoff rank',\n",
    "            'G':  'Games Played',\n",
    "            'OOBP':  'Opponent On-Base Percentage',\n",
    "            'OSLG':  'Opponent Slugging Percentage'\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rf'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "           \n",
    "df_raw = Mb_data\n",
    "rf = RandomForestRegressor(n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000): \n",
    "        with pd.option_context(\"display.max_columns\", 1000): \n",
    "            display(df)\n",
    "            \n",
    "def add_RD(df):\n",
    "    df['RD'] = df.apply(lambda row: row.RS - row.RA, axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First look on DATA and information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw.tail().transpose())\n",
    "print('#'*40)\n",
    "display('Some more info')\n",
    "print('#'*40)\n",
    "display(df_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = df_raw\n",
    "add_RD(df_prep)\n",
    "display_all(df_prep.tail().transpose())\n",
    "display(df_prep.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Team', 'League', 'Year', 'RankSeason', 'RankPlayoffs', 'Playoffs']\n",
    "df_prep = df_prep.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Fix missing values and type\n",
    "df_prep.replace(\"?\",0, inplace=True)\n",
    "#df_prep = df_prep[df_prep.OOBP != 0]\n",
    "df_prep[['OOBP','OSLG']] = df_prep[['OOBP','OSLG']].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_prep.columns.values)\n",
    "display(df_prep.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "def split_simple(df, n): \n",
    "    '''n... number to split at'''\n",
    "    return df[:n].copy(), df[n:].copy()\n",
    "\n",
    "def split_proper(df, test_ratio, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    shuffled_indices = np.random.permutation(len(df))\n",
    "    test_set_size = int(len(df) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return df.iloc[train_indices], df.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for the wins (FTW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.2 # test/num_samples\n",
    "num_instances, _ = df_prep.shape\n",
    "print(f\"From {num_instances} using {num_instances*ratio:.0f} for testing and {num_instances*(1-ratio):.0f} for training. Ratio = {ratio*100:.2f}%\")\n",
    "\n",
    "X, y = (df_prep.drop(['W'], axis=1), df_prep.W)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = ratio, random_state = 42)\n",
    "\n",
    "train_simple, test_simple = split_simple(df_prep, int(num_instances*(1-ratio)))\n",
    "\n",
    "display(test_simple)\n",
    "print('\\n\\n\\t\\t\\t\\t\\t\\tVS\\n\\n')\n",
    "display(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def rmse(x,y): \n",
    "    return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m, X_train, X_valid, y_train, y_valid, score='neg_mean_squared_error'):\n",
    "    res = {\n",
    "        'RMS(train)': rmse(m.predict(X_train), y_train),\n",
    "        'RMS(valid)': rmse(m.predict(X_valid), y_valid)}\n",
    "    if score=='neg_mean_squared_error':\n",
    "        res['Model_Score=r²'] = [np.sqrt(-m.score(X_train, y_train)), np.sqrt(-m.score(X_valid, y_valid))]\n",
    "    elif score=='pos_mean_squared_error':\n",
    "        res['Model_Score=r²'] = [np.sqrt(m.score(X_train, y_train)), np.sqrt(m.score(X_valid, y_valid))]\n",
    "    else:\n",
    "        res['Model_Score=r²'] = [m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res['oob_score_'] = m.oob_score_\n",
    "    display(res)\n",
    "    return res\n",
    "\n",
    "# Feature importance\n",
    "from prettytable import PrettyTable as PT\n",
    "def print_RF_featureImportance(rf, X):\n",
    "    table = PT()\n",
    "    table.field_names = ['Feature', 'Score', 'Comment']\n",
    "    for name, score in zip(X.columns.values, rf.feature_importances_):\n",
    "        print(f\"{name}: {score:.5f}\\t\\t... {col_dict[name]}\")\n",
    "        table.add_row([name, round(score, ndigits=4), col_dict[name]])\n",
    "    print(table)\n",
    "\n",
    "before = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 4\n",
    "rf_W = RandomForestRegressor(n_jobs=n_cores)\n",
    "# The following code is supposed to fail due to string values in the input data\n",
    "rf_W.fit(X_train, y_train)\n",
    "print(\"Before:\")\n",
    "display(before)#\n",
    "print(\"Now:\")\n",
    "before = print_score(rf_W, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_RF_featureImportance(rf_W, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_W_prediction = rf_W.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test-rf_W_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to target RD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = (df_prep.drop(['W', 'RD'], axis=1), df_prep.W)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "display(X_test)\n",
    "before = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping:\n",
    "\n",
    "Bootstrapping: Selecting data from a data to generate a new dataset of the same size by picking WITH replacement.\n",
    "\n",
    "Example:\n",
    "\n",
    "    > DS = [1,2,3,4]\n",
    "    > could turn into \n",
    "    > DS_bootstrapped = [3,2,4,4]\n",
    "    \n",
    "Consequences:\n",
    "\n",
    "- Instances (rows) of the original set can end up duplicated (multiple times) in the resulting dataset.\n",
    "- Some instances are left out entirely (up to 1/3) --> \"Out-Of-Bag Dataset\" (=OOB Dataset)\n",
    "\n",
    "## Using the OOB Dataset\n",
    "\n",
    "The OOB dataset was not used to construct the tree, so we can actually use it to test our tree and gain some insight into the error measure of the tree.\n",
    "This error is called the \"Out-Of-Bag Error\" (OOB error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 4\n",
    "number_of_trees = 1000 # default = 100\n",
    "rf = RandomForestRegressor(n_jobs=n_cores, n_estimators=number_of_trees, bootstrap=True) #, verbose=1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Before:\")\n",
    "display(before)#\n",
    "print(\"Now:\")\n",
    "before = print_score(rf, X_train, X_test, y_train, y_test)\n",
    "print()\n",
    "print(\"Feature importance\")\n",
    "print_RF_featureImportance(rf, X_train)\n",
    "rf_RD = rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfRD_prediction = rf_RD.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test-rfRD_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Hyperparameters via GridSearch\n",
    "\n",
    "because we lazy bois\n",
    "\n",
    "## Notes on the RandomForestRegressor from scikit-learn\n",
    "-----\n",
    "The default values for the parameters controlling the size of the trees\n",
    "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "unpruned trees which can potentially be very large on some data sets. To\n",
    "reduce memory consumption, the complexity and size of the trees should be\n",
    "controlled by setting those parameter values.\n",
    "\n",
    "## Number of variables/features per tree\n",
    "\n",
    "A good starting point is/might be: *the square root of the number of features presented to the tree*. Then, test some values below and above that starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_GridSearchResult(grid):\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt\n",
    "num_features = X.shape[1]\n",
    "print(num_features)\n",
    "sqrt_num_features = round(sqrt(num_features), 0)\n",
    "sqrt_num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "n_cores = 4\n",
    "# but since we dont have that many features...we are just gonna brute force it :D\n",
    "param_grid = [\n",
    "    {\n",
    "        'n_estimators': [3, 10, 30, 100, 1000], 'max_features': [i for i in range(1,num_features+1)]\n",
    "    }\n",
    "#,{'bootstrap': [False], 'n_estimators': [3, 30, 100, 1000], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "k = 10\n",
    "forest_reg = RandomForestRegressor(n_jobs=n_cores)\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, n_jobs=n_cores , cv=k, return_train_score=True) #, scoring='neg_mean_squared_error'\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_GridSearchResult(grid_search)\n",
    "print_score(grid_search, X_train, X_test, y_train, y_test, score='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max_features = 8\")\n",
    "{'RMS(train)': 1.6169133890268252,\n",
    " 'RMS(valid)': 4.158368280598173,\n",
    " 'Model_Score=r²': (0.980355161825956, 0.8600920766278795)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Run', 'Score']\n",
    "    for i, score in enumerate(scores):\n",
    "        table.add_row([i, round(score, 3)])\n",
    "    print(table)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "model = rf_RD\n",
    "scores = cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_scores(rf_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump model\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "joblib.dump(rf_RD, \"tmp/rf_RD.pkl\")\n",
    "# To load the model\n",
    "# my_model_loaded = joblib.load(\"my_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary on Random Forests\n",
    "\n",
    "Book I like: **Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow** by Aurélien Géron (my colleague form work recommended it to me)\n",
    "\n",
    "Youtube series heavily based on that book: https://www.youtube.com/watch?v=D_2LkhMJcfY\n",
    "\n",
    "The company behind the Youtube channel kinda sucks...but the videos are a nice summary of the book.\n",
    "\n",
    "git: https://github.com/ageron/handson-ml\n",
    "\n",
    "citation: Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. Retrieved from https://books.google.at/books?id=HHetDwAAQBAJ\n",
    "\n",
    "Another video series that seemed nice: https://www.youtube.com/watch?v=J4Wdy0Wc_xQ\n",
    "\n",
    "Scikit-Learn: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=random%20forest#sklearn.ensemble.RandomForestRegressor\n",
    "\n",
    "Another series of courses I use: https://course18.fast.ai/lessonsml1/lesson1.html\n",
    "\n",
    "    > github: https://github.com/fastai/fastai/tree/master/courses/ml1\n",
    "    > \"Mitschrift\": https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-1-84a1dc2b5236\n",
    "\n",
    "Analysis/Guide on the Moneyball-Set: https://www.kaggle.com/wduckett/beane-and-depodesta-s-regression-roadmap\n",
    "\n",
    "## General idea:\n",
    "\n",
    "Based on decision trees (aka Classification And Regression Tree = CART).\n",
    "Use multiple (different) CARTs and use a reduced version of each trees output (say, some form of average)... \"Wisdom of the crowd\".\n",
    "\n",
    "Growing a decision tree: Use every single feature for the tree --> search for the very best feature when splitting a node.\n",
    "\n",
    "The Random Forest algorithm introduces extra randomness when growing trees; \n",
    "instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features.\n",
    "This results in agreater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model. \n",
    "\n",
    "They are a simple examples of \"Ensemble Learning\"; using multiple predictors to form another predictor.\n",
    "In general, the set of base predictors can be made up of different types and/or use different sets of hyperparameters.\n",
    "The result of the Ensemble is calculated by aggregating the result of each base predictor (e.g. by voting or averaging).\n",
    "\n",
    "## Pseudocode\n",
    "\n",
    "Training:\n",
    "\n",
    "1. Assume number of cases in the training set is N. Then, a sample of these N cases is taken at random but with replacement (bootstrapping).\n",
    "\n",
    "2. If there are M input variables (or features), a number m < M is specified (subset of features) such that at each node, m variables are selected at random out of the M. The best split on these m is used to split the node. The value of m is held constant while we grow the forest.\n",
    "\n",
    "3. Each tree is grown to the largest extent possible and there is no pruning.\n",
    "\n",
    "Prediction:\n",
    "\n",
    "1. Let each tree produce its prediction output.\n",
    "\n",
    "2. Aggregate the individual prediction into the final result of the Random Forest (i.e. majority vote for classification, average for regression)\n",
    "\n",
    "Terminology: **B**ootstrapping data + **agg**regating the results to make a decision = Bagging\n",
    "\n",
    "## Advantages:\n",
    "\n",
    "    - can handle both regression and classification\n",
    "    - handles missing data well --> less preprocessing needed\n",
    "    - maintains data accuracy\n",
    "    - won't overfit (surprising as CARTs tend to do that)\n",
    "    - can handle large amounts of data with high dimensionality well\n",
    "    - usefull for EDA: feature importance\n",
    "    \n",
    "## Disadvantages:\n",
    "\n",
    "    - not AS great for regression because it doesn't actually give continous output\n",
    "    - little control over what the model does (black box approach)\n",
    "    \n",
    "## Applications:\n",
    "\n",
    "Think of areas where similiar structures are already used by domain experts:\n",
    "\n",
    "    - Medicine: diagnosing, figuring out medication,...\n",
    "    - Stock market\n",
    "    - Image classification (XBOX Kinect body part identification)\n",
    "    \n",
    "## How to tune the Hyperparameters?\n",
    "\n",
    "https://www.gormanalysis.com/blog/random-forest-from-top-to-bottom/\n",
    "\n",
    "On other words, “How do I tune the hyperparameters of a random forest?” This question isn’t specific to random forest. The most common approach is to use grid-search + cross validation – essentially “guess and check” where the training phase is based on one dataset and the testing phase is based on another. Otherwise, here are some notes specific to random forest:\n",
    "\n",
    "- The more trees the better. But at a certain point the next tree just slows down your computer without adding more predictive power\n",
    "- Leo Breiman (random forest’s creator) suggests sampling (with replacement) n rows from the training set before growing each tree where n = number of rows in the training set. This technique is known as bagging and will result in roughly 63% of the unique training samples being used to construct a single decision tree.\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "neighbors.kNeigh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "df_raw.to_feather('tmp/bulldozers-raw')\n",
    "df_raw = pd.read_feather('tmp/raw')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
